{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "os.nice(10)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"10\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import astra\n",
    "import numpy as np\n",
    "import pydicom \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct dataset for U-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define the astra projection function \n",
    "\n",
    "\n",
    "def astra_project(src_dir, file_list, n_projections, savedir):\n",
    "    \n",
    "    \"\"\"\n",
    "    Forward and backproject CT images with a specified number of projections. \n",
    "    Saves the output in the specified savedir folder with the same filename as the original file.\n",
    "    \n",
    "    Parameters\n",
    "    \n",
    "    -----------------\n",
    "    src_dir : string\n",
    "        directory where base images are located\n",
    "    file_list : list of strings\n",
    "        list of filenames which shall be processed\n",
    "    n_projections : int\n",
    "        number of projections to be used for the reconstruction\n",
    "    savedir : str\n",
    "        directory where the output files are saved\n",
    "    \"\"\"\n",
    "    \n",
    "    #define geometry\n",
    "    geometry = 'parallel'\n",
    "    pixel_spacing = 1\n",
    "    n_detectors = 768\n",
    "\n",
    "    #iterate through list\n",
    "    for i, file in enumerate(file_list):\n",
    "        sys.stdout.write('\\r'+f'{i}/{len(file_list)}')\n",
    "        \n",
    "        #load file\n",
    "        pyd = pydicom.dcmread(src_dir + \"/\" + file)        \n",
    " \n",
    "        #setup geometries\n",
    "        vol_geom = astra.create_vol_geom(512, 512)\n",
    "        proj_geom = astra.create_proj_geom(geometry, pixel_spacing, n_detectors, np.linspace(0,2*np.pi,n_projections,False))\n",
    "\n",
    "        proj_id = astra.create_projector('cuda', proj_geom,vol_geom)\n",
    "        data = (pyd.RescaleSlope*pyd.pixel_array + pyd.RescaleIntercept + 1024)/4095\n",
    "\n",
    "        if data.shape == (512, 512):\n",
    "            sinogram_id, sinogram = astra.create_sino(data, proj_id, returnData=True, gpuIndex=0)\n",
    "\n",
    "            # Create a data object for the reconstruction\n",
    "\n",
    "            rec_id = astra.data2d.create('-vol', vol_geom)\n",
    "            \n",
    "            # create configuration\n",
    "            cfg = astra.astra_dict('FBP_CUDA')\n",
    "            cfg['ReconstructionDataId'] = rec_id\n",
    "            cfg['ProjectionDataId'] = sinogram_id\n",
    "            cfg['option'] = {'FilterType': 'Ram-Lak'}\n",
    "            alg_id = astra.algorithm.create(cfg)\n",
    "            astra.algorithm.run(alg_id)\n",
    "            rec = astra.data2d.get(rec_id)\n",
    "        \n",
    "            # Clean up.\n",
    "            astra.algorithm.delete(alg_id)EfficientNet\n",
    "            astra.data2d.delete(rec_id)\n",
    "            astra.data2d.delete(sinogram_id)\n",
    "            \n",
    "            #save data\n",
    "            name = os.path.splitext(os.path.basename(file))[0]\n",
    "            print(name)\n",
    "            savepath = savedir + f'/{angle}/'\n",
    "            if not os.path.isdir(savepath):\n",
    "                os.makedirs(savepath)\n",
    "                print(\"\\ncreated folder: \", savepath)\n",
    "            np.save(savepath + f'{name}.npy', rec.astype('float16'))\n",
    "        else:\n",
    "            print('invalid shape, skip this image')\n",
    "            \n",
    "    astra.projecto.delete(proj_id)\n",
    "    \n",
    "    return data, sinogram, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure that the base dicom files are located in ./Data/stage_2_train/\n",
    "#get list of training and test data\n",
    "df_train = pd.read_csv(os.path.abspath(\"./train_UNet.csv\"))\n",
    "df_test = pd.read_csv(os.path.abspath(\"./test_data.csv\"))\n",
    "file_list = [*df_test['filename'], *df_train['filename']]\n",
    "\n",
    "src_dir = os.path.abspath(\"./Data/stage_2_train/\")\n",
    "save_dir = os.path.abspath(\"./Data/\")\n",
    "angles = [4096, 2048, 1024, 512, 256, 128, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data\n",
    "for angle in angles:\n",
    "    print(\"\\n\", angle)\n",
    "    data, sino, rec = astra_project(src_dir, file_list, angle, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "\n",
    "name = os.listdir(save_dir + \"/4096/\")[0]\n",
    "print(name)\n",
    "for angle in angles:\n",
    "    img = np.load(f\"{save_dir}/{angle}/{name}\")\n",
    "    plt.figure(f\"{angle}\")\n",
    "    plt.imshow(img*4095-1024, vmin=0, vmax=80, cmap='gray')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct dataset for EfficientNet training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the following functions, path, df_meta and save_path have to be defined outside the functions in order for multiprocessing to work\n",
    "#this is probably not the best practice but multiprocessing.starmap and functools.partial got stuck\n",
    "\n",
    "def load_rescale(filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Load dicom file -> rescale with slope and intercept -> \n",
    "    clip to brain window -> scale to [0, 255] -> resize to (260, 260)\n",
    "    \n",
    "    Parameters\n",
    "    -----------------\n",
    "    filename : str\n",
    "        filename of dicom file\n",
    "        \n",
    "    Returns\n",
    "    -----------------\n",
    "    numpy array\n",
    "    \"\"\"    \n",
    "    print(\"load_rescale1\")\n",
    "    file = pydicom.dcmread(path + filename)\n",
    "    array = file.RescaleSlope*file.pixel_array + file.RescaleIntercept\n",
    "    array = array.clip(0, 80)/80.* 255.\n",
    "    array = tf.image.resize(array[tf.newaxis, ..., tf.newaxis], [260, 260])\n",
    "    return np.array(array).astype('uint8').squeeze()\n",
    "    \n",
    "def load_neighbouring_slices(slice_id):\n",
    "    \n",
    "    \"\"\"\n",
    "    Forward and backproject CT images with a specified number of projections. \n",
    "    Saves the output in the specified savedir folder with the same filename as the original file.\n",
    "    \n",
    "    Parameters\n",
    "    -----------------\n",
    "    slice_id : string\n",
    "        slice id of file\n",
    "        \n",
    "    Returns\n",
    "    -----------------\n",
    "    image_cat : array\n",
    "        concatenated image with channels\n",
    "    label : str\n",
    "    filename : str\n",
    "    \"\"\"\n",
    "    #print(slice_id)\n",
    "    index = int(slice_id.split('_')[2])\n",
    "    study_uid = 'ID_' + slice_id.split('_')[1]\n",
    "    df_copy = df_meta.copy()\n",
    "    uid_df = df_copy.loc[df_copy['study_instance_uid'] == study_uid]\n",
    "\n",
    "    if index == 0:\n",
    "        index = 1  \n",
    "    if index == (uid_df.shape[0] -1):\n",
    "        index = index-1\n",
    "    #print(uid_df)\n",
    "    filename_down = uid_df.loc[uid_df['slice_id'] == study_uid + f'_{index - 1}']['filename'].values[0]\n",
    "    filename = uid_df.loc[uid_df['slice_id'] == study_uid + f'_{index}']['filename'].values[0]\n",
    "    filename_up = uid_df.loc[uid_df['slice_id'] == study_uid + f'_{index + 1}']['filename'].values[0]\n",
    "    #print(filename_down, filename, filename_up)\n",
    "\n",
    "    image_down = load_rescale(filename_down)\n",
    "    image_up = load_rescale(filename_up)\n",
    "    image = load_rescale(filename)\n",
    "    image_cat = np.concatenate([image_up[:,:,np.newaxis], image[:,:,np.newaxis], image_down[:,:,np.newaxis]], 2)\n",
    "    label = uid_df.loc[uid_df['filename']==filename].loc[:, 'any': 'subdural'].values\n",
    "    return image_cat, label, filename\n",
    "\n",
    "def save_file(slice_id):\n",
    "    \"\"\"\n",
    "    wrapper function for loading, concatinating and saving image\n",
    "    \"\"\"\n",
    "    img_cat, label, filename = load_neighbouring_slices(slice_id)\n",
    "    np.save(save_path + filename.replace('.dcm', '.npy'), img_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define path, load csv\n",
    "path = os.path.abspath(\"./Data/stage_2_train/\")\n",
    "df_meta = pd.read_csv(os.path.abspath(\"./train_EfficientNet.csv\"))\n",
    "slice_ids = df_meta[\"slice_id\"]\n",
    "save_path = os.path.abspath(\"./Data/\") + \"/dataset_neighbouring/\"\n",
    "if not os.path.isdir(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    print(\"\\ncreated folder: \", savepath), repeat(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let loop run and save files\n",
    "with Pool(5) as p:\n",
    "    p.map(save_file, slice_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "for i in range(1, 4):\n",
    "    load = np.load(save_path + os.listdir(save_path)[i])\n",
    "    print(\"min:\", load.min(), \"mean:\", load.mean(), \"max:\", load.max())\n",
    "    plt.figure()\n",
    "    plt.imshow(load)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
